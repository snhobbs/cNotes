Heuristics \ra efficient algos that get a good although not necessarily perfect solution
\chapter{Algorithms}

\rsrc{NIST Dictionary of Algos and Data Structs}\href{https://xlinux.nist.gov/dads/}{NIST Link}

\section{Asymptotics}
How the algorithm grows as N \ra $\infty$.

\rsrc{Algorithms by David Sedgewick} Page 67

\rsrc{David Mount Notes}\href{http://www.cs.umd.edu/~mount/251/Lects/251lects.pdf}{Link}

\rsrc{Big-O Notation}\href{https://en.wikipedia.org/wiki/Big_O_notation}{Wiki Link}

\rsrc{Time Complexity}\href{https://en.wikipedia.org/wiki/Time_complexity#Constant_time}{Wiki Link}

\subsection{Notation}
	\textbf{$\Theta$}: The asymptotic class of an algo. 
	\begin{equation}
		\Theta(g(n)) \equiv \bigg\{f(n), 0<=c_1g(n)<=f(n)<=c_2g(n) | c_1, c_2, n_0 \in |\Re| \textrm{ and } n_0 <= n \bigg\}
	\end{equation}
	For a algo to be $\Theta$(g(n)) it needs to be both O(g(n)) and $\Omega$(g(n)). A $\Theta$(g(n)) grows at exactly g(n).

	\hspace{11pt}
	
	\textbf{O}: Upper asymptotic bound for an algo. An algo that has O(g(n)) grows at or slower than that rate. 
		\begin{equation}
			O(g(n)) \equiv \bigg\{f(n) | 0<= f(n) <=cg(n) | c, n_0 \in |\Re| \textrm{ and } n_0 <= n \bigg\}
		\end{equation}

	\hspace{11pt}

	\textbf{$\Omega$}: The lower bound on the growth. An algo w/ $\Omega$(g(n)) grows at or faster than g(n). 
		\begin{equation}
			\Omega(g(n)) \equiv \bigg\{f(n) | 0 <= cg(n) <= f(n) | c_1, c_2, n_0 \in |\Re| \textrm{ and } n_0 <= n \bigg\}
		\end{equation}

		\subsection{Performace}
		Some algorithms, classically quicksort, have drastically different average and worst case performance. The best case is seldom used.
		\begin{description}
			\item[Worst Case: ] $T_{worst}(n) \equiv max_{|I|=n}T(I)$. The worst possible performance with legal input.
			\item[Best Case: ] The fastest performance \ra a sorting algorithm realizing its already sorted
			\item[Average Case: ] $T_{avg}(n) \equiv \sum_{|I|=n} p(I)T(I)$ Where p(I) is the probability weight of T(I) occuring.
		\end{description}

\subsection{Asymptotic Analysis}
\begin{table}
	\caption{Asymptotic growth types}
	\label{table:asymtoticGrowth}
	\begin{tabular}{llp{.5\textwidth}}
\hline\hline
Notation& Name& Example\\\hline
O(1)&Constant Time&Seeing if a binary number is even or odd\\
O(log log n) &Double Logarthmic& \\
O(log n)& Logarithmic& Finding and item in a sorted array with binary search\\
O( (log n)$^c$ ) w/ $c>1$& Polylogarithmic& \\
O(n$^c$) w/ $0<c<1$&Fractional power &Searching in a kd-tree \\
O(n)& Linear& Find an item in an unsorted list\\
O(n log*n)& n log-star n& Union-find\\
O(n log n)& quasilinear/linearithmic&Theoretical limit on sorting based on comparison (heapsort, mergesort, quicksort)\\
O(n$^2$)& Quadratic&Simple comparison sorting (bubble, selection, etc)\\
O(n$^c$)& Polynomial& LU decomposition\\
O(2$^n$),O(n$^n$),O(n!)&Exponential&\\
\hline\hline
\end{tabular}
\end{table}

\subsubsection{(Strong) Induction}
\subsubsection{Iteration}
\subsubsection{Recurrance}
\subsubsection{Bounding}
\subsubsection{Integration}
\subsection{Master's Theorem}
\rsrc{Masters Theorem}\href{https://en.wikipedia.org/wiki/Master_theorem}{Wiki Link}

\rsrc{Akra-Bazzi Method}\href{https://en.wikipedia.org/wiki/Akra%E2%80%93Bazzi_method}{Wiki Link}


	For divide and conquer algos. The generalization of this is the Akra-Bazzi method
	Let a $>=$ 1, b$>$ 1 be constants and let T(n) be the recurrence:
	\begin{equation}
		T(n) =  aT(n/b) = N^k    n>=0
	\end{equation}
	Assume n is a power of b and the basis case T(1) be a constant. Then:
		\begin{description}
			\item[Case 1:] if $a > b^k$ then T(n) $\in \Theta(n^{log_b a})$
			\item[Case 2:] if $a == b^k$ then T(n) $\in \Theta(n^k log n)$
			\item[Case 3:] if $a < b^k$ then T(n) $\in \Theta(n^k)$
		\end{description}

		Merge sort has a=2, b=2, k=1 so $T(n) \in \Theta(n log n)$
		Binary search a=1, b=2, k=0 so $T(n) \in \Theta(log n)$


\section{Algorithm Types}
\subsection{Divide \& Conquer}
\subsection{Selection}

\section{Searching}
\subsection{Binary Search}

\rsrc{Binary Search}\href{https://en.wikipedia.org/wiki/Binary_search_algorithm}{Wiki Link}

\subsection{Linear Search}

\rsrc{Linear Search}\href{https://en.wikipedia.org/wiki/Linear_search}{Wiki Link}

\section{Selection}
\subsection{Sieve Technique}

\section{Sorting}
\subsection{Merge Sort}
\subsection{Heap Sort}
\subsection{Quick Sort}
\subsection{Bubble Sort}
\subsection{Insertion Sort}
\subsection{Selection Sort}
\subsection{Count Sort}
\subsection{Radix Sort}
\section{Peak Finding}
\subsection{Simplex}

